Step 1: Creating the S3 bucket
---------------------------------
1.Go to the AWS console  -> S3 bucket ->  Create bucket

*AWS region -> will be taken as per your current place
  - Data will be replicated across multiple AZ's  in this region 
  - Choose the region closer to you or the customer so you will get (Best latency and cheapest for you )

*Bucket type -> General purpose   -> Strandard S3 bucket ,multi-AZ ,Support all storage class
              |_Directory         -> Single AZ, Ultra-low latency , Higher cost

*Bucket Name -> Global name ,used in URL's and API'S 
              |_ No lowercasee,no space ,cannot change later 

*Copie the setting from the existing bucket  : [Only the configuration not the data ]

*Object ownership 
  - ACL's disable  -> YOur account own's all objects and permission via IAM and bucket policies
  - ACzL's enabled -> Objects in thiis bucket can be owned by other AWS account .Access to this bucket and its 
                      objects can be spcificed using ACL's 
                       if you enable this you will be given two options  will pop up 
                       - Bucker owner preffereed  -> If new objects written to this bucket specify the bucket-owner-full-control canned ACL, they are owned by the bucket owner. Otherwise, they are owned by the object writer.
                       - Object writer  -> the object writer remains the object owner
*Block the public access setting to this bucket ->
  - Here you can directly block all public access 
                      or 
  - allow  some access throught the polices 


  Bucker versioning :
                     |_ disable it -> Only the latest version is stored, delete = gone 
                      |_ enable it  -> Keeps all version's  ,protects against overwrite and delete
                                        - Required for the Object lock and S3 backup and PITR
                                         - Increase the storage cost 

Tags :
Meaning : key - value metadata for cost tracking ,Organizatoin ,Automation 

Default encryption : 
 - The Encryption applies automatically to all new objects
    SSE-S3 (Amazon S3 managed keys)  - free
    SSE-KMS                           - uses AWS KMS keys  and Extra cost
    DSSE-KMS                          - Dual encryption ,higher security and higher cost

  Bucket key (Only for SSE KMS)
  -------------------------------
  Reduces KMS call's  -> lower cost
Object Lock
Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock works only in versioned buckets
  
METADATA : 
This feature lets S3 automatically create a database-like  view of your buket content 

Helps to 
*Track every object
*Track every changes 
*Stores that info as queryable table .So you can query with SQL 

What happens is ??
AWS creates managed metadata tabels behind the scenes:
*Stored in another AWS-managed S3 bucket
*Format : Apache Iceberg
*Continuously update ,You do NOT need to manage these tabels manually 


Metadata configuration :
Once enabled :
*Every upload / delete / overwrite is tracked 
*Metadata tabels auto-update

What can you do with it :
*You can Run SQL queries using : 
  - Amazon Athena 
  - Qucick sight 
  - Apache spark 

Ex:
SELECT * FROM bucket_metadata
WHERE size > 100MB
AND last_modified > now() - 7 days;

Types of metadata : 

1.Journal Tabel
-----------------------
What it stores: 
*Object changes over time 
*Upload 
*Deletes 
*UPdates

Use cases:
*Auditing 
*Security 
*Compliance
*Who changed what and when 

Think of it like a cloudTrial of the S3 objects

Record expiration : 
*Disable : History is kept forever 
*Enable  : auto delete old records (cheaper )

2.Live inventory Tabel 
-------------------------------
What it stores
*Current state of ALL objects
*One row per object

Use cases: 
*Big data jobs
*ML pipelines
*Batch processing 
*Finding files quickly

Think of it like a :
>> ls -l for your bucket  - but instant and queryable

Keep the encryption default unless the job needs strict security and compliance

______________________________________________________________________________________________________________
ACCESS POINT :
An S3 Access point is an separate ,named way to access an S3 bucket with its own permission and network rules \
Used for API access such as (CLI/SDK/backend apps)
It not for the static website


It doesn't store the data 
It Controls  how the data is accessed 

Normal bucket access: 
---------------------------------
 App / User
    |
 S3 bucket

*Only bucket policy 
*One big set of permission 
*Hard to manage  when many app's / users exits

Bucket access with Access point 
--------------------------------

App A â†’ Access Point A â†’ Bucket
App B â†’ Access Point B â†’ Bucket

*Same bucket 
*Multiple access point 
*Each access point  : 
  - has its own  Access policy 
  - has its own  Network policy 

Access point configuration :
1.Give the accesss point a name:   
2.Network origin 
  Here you have two options: 
  - Virtual private cloud ->  You should give the VPC ID   [ No internet access .Request are made over a spcidfied VPC only ]
  - Internet              ->  The access point has a public endpoint  .Can be accessed from the Intenet ,AWS cli ,SDK's 
                               -Still not public by default 
                               - Requires authentication (IAM/ signed request)
3.Block Public Access Settings 
You selected ; 
Block all public access 
This means : 
*Nobody can make this access point public
*Even if someone tries to add:
-Public ACLs
-Public policie

Access Point policy (optional)
-------------------------------------
This is a JSON policy that controls:
*Who can access the bucket
*What they can do
*ONLY via this access point

Example use cases:

- Read-only access
- Upload-only access
- Restrict to one IAM role

If you leave it empty:

*Access is controlled by:
-IAM permissions
-Bucket policy

ðŸ“Œ Access is allowed only if BOTH allow it
_____________________________________________________________________________________________________________
MANAGEMENT
-------------

Lifecycle rules
-----------------
Lifecycle rules tells the S3 what to do with objects automatically as they get older  
- move them to cheaper storage or delete them 

LifeCycle rule name:
---------------------
Name: Lifecycle rule name
=> Give the name that suits 

Choose the rule scope :
This decides which object the rules applies

*Apply to all objects in the buckets
 - Every file in the bukcet
 - Simple but dangerous if used carelessly 

*Limit the scope using the filter 
 -You an target specific target 

Filter type: 
Prefix:[Ex : logs/,uploads/,temp/]
Meaning : only the objects whose key start with that prefix
Ex: 
- logs/app.logs
- logs/2026/a.logs
- images/pic.jpg   -> this will not be included as it doesn't start with any of the prefix

Object tags: 
----------------
env = dev
type  = temp

Meaning object with thesse tags  ,this allow a very precise  control  .Used a lot in production 

Object size:
-------------------
*Minimux / Maximum size of the object
 Useful for :
  *large backup 
  *small temp files

Lifecycle rule action : Choose the actions you want this rule to perform 

1.Transition  current version of objects between the storage classes
   Means : choose the transition to move current version's of objects between storage classes based on your u
            use case scenario and performance access requirementes .These transitoins start from when the objects
             are created and ocnsecutively applied 
 
This will ask for a Storage class you want to transition to  and the days after which this data will be  transitioned
2.Transition non currnet  version's of objects between storage classes [This will applie for non-versioned objects ]
3.Expire  current version of objects 
  Means : if the version is enabled  then when the expiration date is reached  
          1. S3 does not delete the object data
          2. s3 adds a delete marker 
          3.The object disappears  form the console, Normal GET , Website access 
             But 
                *The actual files become a non-current version 
                *You are still charged for the storage 
         Expire the current version and permanently delets the non current version 

          if the versioning is disabled  and the expiration data is reached 
          1.Object is permanently deleted
          2.NO delete marker 
          3.No recover
          4.No storage cost after deletion 
This option will ask for the  day's to keep the object from deletion 
4.Permanently delete noncurrent version of objects
  Choose when amazon S3 permanently deletes speified non-current  version objects 
  This will ask for number of days after which the objects become noncurrent and the number of newer version to be retained 
5.Delete expired objects delets markers or incomplete multipart uploads
  Delete expired objects delete marker 
  
When you enable this: 
  *S3 automatically removes those useless delete marker
  *Object metadata is cleaned up 
  *List the operation become faster
  *No data loss 
  
 Incomplete mutlipart uploads 
  *While uploading the large file we often split it into part and uplaod  
  *While uplaoding through parts if the upload fail or interrupted ro abandoned  the corrupted part will stay in the s3 and you will be charged for it
 But when you  enable this  deletes multipart uploads that have been incomplete for X days
_________________________________________________________________________________________________________________
Replication 

S3 replication  = automatic  copying of the objects from one bucket to another 
It works : 
*Within the same region  (SRR - Same region replication )
*Across region           (CRR Cross regio replication )

Whenever an object is : 
*Uploaded
*Updated
*Deleted 
S3 automatically copies that change to the destination bucket
** This replication only works when  the versioning is enabled so that its easy to trackt the changes 

Configuring the  Replication Rule screen : 
1.Replication rule Name :  Give a name for the replication rule
2.Choose wheather the  rule will be enabled or disabled  when created 
  Means : Enabled -> replication start's immediately 
          Disabled -> rule exits but does nothing 
3.Prioirty:
  Used only if : 
  *You have multiple replication rules 
  *Same object matches more that one rule 
Lower the number higher the priority 
 
4. The Rule Scope is similar to lifecycle
5.Destination section : 
  Where the copy goes:*same account, different account 
6.IAM role : 
   Provide  permission to access the spcified resources 
7.Encryption : Inside this you have the option to 
               Enable : Replicate objects  encrypted with AWS key management services 
8.Destination storage class : 
   Change the storage class of the s3 where the replicated data will be kept 
9. Additional replication options :
   Replication Time Control (RTC)
   - Replication Time Control replicates 99.99% of new objects within 15 minutes and 
      includes replication metrics. Additional fees will apply. 
    Delete marker replication
    - Delete markers created by S3 delete operations will be replicated.
      Delete markers created by lifecycle rules are not replicated.
    Replica modification sync
    - Replicate metadata changes made to replicas from the destination bucket to the source bucket.
    Replication metrics
    - With replication metrics, you can monitor the total number and size of objects that are pending replication,
      and the maximum replication time to the destination Region. 
      You can also view and diagnose replication failures. CloudWatch metrics fees apply.


____________________________________________________________________________________________________________________________

S3 Inventory 
--------------
S3 Inventory  = a periodic  report (CSV/Parquet/ORC) listing objects in your bucket 
                AWS generates this automatically and drops the report into another S3 bucket

Meaning : "Give me  an Excel sheet of everything inside my bucket "



Why does this exits????
--------------------------
Because : 
*aws s3 ls is slow  for millionn of objects 
*YOu can't easily query objects metadata at  scale
*Enterprises need audit ,cost analysis ,compliance

This S3 inventory gives you all the things about the S3 like:
Object list,Size,Storage class,Encryption,Replication status,Object Lock info,Ownership,Lifecycle dates

All in one file like a  schedule 


Inventory configuration 
--------------------------------------
1.Inventory configuration name:  Give a name for this inventory 
2.Invetory scope only : 
 - You can limit the scope of this configuration to a single prefix
      :Write the prefix you want the inventory to look into 
Object version : 
- Current version only 
- Inclued all version 
3.Destination bucket 
  - This is where the inventory report  file will be saved . The destination bucket must be  in the same AWS 
    region as the source bucket

    But there is no restirction on the AWs account it can be stored in other AWS account also 
    Format : s3://inventory-reports-bucket/reports/

    >> AWS will automatically write the files here 

4.Destination Bucket permission : 
   >> This policy allows the AWS service to put inventory report files into my bucket 
5.Frequency: Choose how often the report will be generated 
   >> Daily  : The first report will be delivered within 48 hours
   >> Weekly : The first report will be delivered within 48 hour and subseqenrt report's
               will be delivered on sunday 
6.Output format: Choose an output format based on the number of objects  that you expect to list or the analysis 
                  tools that you want to use 
  >> SUCH AS 
    *CSV
    *APACHE ORC
    *APACHE PARQUET 
7. Status: Choose wheather the configuration will be enabled to publish invertory reports
   >> Disable   - config exits but does nothing 
      Enable    - report start generating
8.Inventory report encryptoin: Server-side encryption protects the data at rest 
  >> Options: 
      - Don't specify an encryption key 
          --The bucket setting for default encryption are used to encrypt when storing then in Amazon S3 
      -Specify an encryption key 
          -- The specified encryption key is used to encrypt objects before storing them in Amazon S3
9.Additional  metadata fields : This decides what coloums appear in the report
  Exmple: 
  *Object size
  *Last modified
  *Storage class
  *Encryption
  *Replication status
  *Object Lock info
  
More field = Bigger report = more cost 

_______________________________________________________________________________________________________________
METRICS
Storage Class analysis 
-----------------------
This helps in setting up storage analytics on your S3 bucket 
You can control which objects are analyzed (Prefix/tags) ,and optionally export daily CSV reports 

Configuration of the Storage class analysis:
--------------------------------------------
1.Configuration Name : The name of this configuration [Cann't be changed later]
2.Configuration scope : This tell the AWS which objects you want to analyze 
                       >>  Apply to all the objects
                       >> Limit the scope using filter's such as Prefix ,Object tags , or both
3.If you choose the second option you should enter the prefix  or the object tags  ro combination of both to 
  to filter 
4.Export the CSV: Specify an S3 bucket to store CSV exports .A file will  be exported every 24 hours
   >> If you enable it then you should give the S3 bucket location 
   >> If not not need it will be disabled 
_______________________________________________________________________________________________________________
PERMISSION 

1.Block Public Access
----------------------------
PURPOSE: To prevent  accidental  public exposure of your bucket or objects  

If you enable " Block all public access = ON "
*Nobody on the internet can access your bucket
*Even if you write a  bucket policy allowing public access  -> S3 will block it 
If you disable " Block all public access = OFF "
*You can allow public access if needed

2.Bucket Policy 
----------------------------
This is the main accesss control system in S3

Bucket policy decides : 
*Who can read the files 
*Who can write the files
*Can the public access it 
*Can the cloudfront access it 
*Can another AWS account access it 

Bucket policies are powerfull and flexible 
Bucket policies don't  apply to objects owned by other accouts 


3.Object OwnerShip
------------------------------
This is the  important setting that controls who owns uploaded objects 
You saw: 
Bucket owner enforced : 
>> A C L   are disabled
  All the objects in this bucket are owned by this account 
  Access to this buket and its object is spcified using only policies 

    This removes legacy behaviour where the uploader owned the object 

Why this is good??
*No permission conflicts
*No ACL confusion 
*The bucket owner alwayys has full control 

If the ACL is enabled :
>>   A C L  is enabled 
     Objects in this bucket can be owned by other AWS accouts  .
     Access to this bucket and its objects can be specified using ACL's 

If you enable this there will be two other optoins under the Object ownership :
>> Bucket ownership prefered  : If someone uploads an object and they include a special ACL [bucker-owner-full control]
                                the bucker owner will own the object else the object writer own the object 
    
>> Object writer              : The object owner become the owner ,ALWAYS 

4.ACL  - Accesss contorl lists (old systems)
-----------------------------------------------
ACL existed before bucker policies 
Since our bucket is bucket owner enfored ,ACL are actually ignored 

Should you use ACL'S ?? NO
*ACLs are outdated 
*Use bucket policies instead 

5.CORS (Cross-Origin Resource sharing )
-------------------------------------------------
CORS allow the website hosted on one domain to access your S3 bucket

Example  : 
Your React fronted (example.com) want's t fetch an image from S3
You  need CORS LIKE: 
{
  "AllowedOrigins": ["https://example.com"],
  "AllowedMethods": ["GET"],
  "AllowedHeaders": ["*"]
}
if you don't set CORS?
Browsers will block cross domain requests

___________________________________________________________________________________________________________________________
PROPERTIES
-----------

1.Bucker Overview:
----------------
Gives details about the 
*AWS region 
*Amazon Resource Name
*Creation date
2.Bucker Versioning 
------------------------
- Bucket versioning 
*Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning 
to preserve, retrieve,and restore every version of every object stored in your Amazon S3 bucket.
With versioning, you can easily recover from both unintended user actions and application failures
- Multi-factor authentication(MFA) 
An additional layer of security that requires multi-factor authentication for changing Bucket Versioning settings
and permanently deleting object versions. To modify MFA delete settings, use the AWS CLI, AWS SDK,
or the Amazon S3 REST API.
3.Static Website Hosting 
----------------------------
Static website hosting :Use this bucket to host a static website  or redirect reponses if enabled  
Hosting type : 
 -Host a static website
 -Redirect requests for an object
If the first option is chooosen  you should upload the two files
>>Index.html 
>>Error.html
Redirection rules: [Optional]:Redirection rules ,written in JSON ,automatically redirect webpage requets for spcific content 
If you choose the second options you should give the host name
>> means .target bucket website address or personal domain 
Protocol :[optional]
-none
-http
-https
4.Requester Pays 
------------------------------
When enabled ,the requester pays for requests and data transfer costs and anonymous access to this bucket
is disabled
Requester pays
Disable
 >> Bucket owner pays the cost of data requests and downloads from this bucket.
Enable
 >> Requester will pay for requests and data transfer. While requester pays is enabled, 
    anonymous access to this bucket is disabled.
5.Object Lock
--------------------------------
Store object using write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten 
for a fixed amount of timeor indefinitely .Object lock works only in versionec buckets 

If enabled you will ssee thesse options

Default retention
Automatically protect new objects put into this bucket from being deleted or overwritten.

Disable
Enable    -> Tick

Default retention mode
Governance
Users with specific IAM permissions can overwrite or delete protected object versions during the retention period.

Compliance
No users can overwrite or delete protected object versions during the retention period.

Default retention period.
Enter a number 
6.Transfer acceleration
----------------------------------
Transfer acceleration : Use an accelerated endpoint for faster data trasfer 
Transfer acceleration :  if enabled you will get a endpoint 
7.Amazon EvnetBridge
-----------------------------------
Amazon EventBrige is a service that recieves events from many AWS  services (including the S3) and lets
you route them to other services like 
*Lambda,SNS,SQS,Step functoins,Ec2
If you turn this option ON :  S3 will send all buckets event  from creation to deletion ,from 
replicaton to restoring  to the Event bridge  and the event bridge can trigger the action based on those events
8.Event Notification : 
----------------------------
S3 event notification lets s3 automatically notify another AWS service when something happen in your bucket
When a file is uploaded â†’ trigger Lambda
When a file is deleted â†’ send message to SQS
When object is restored â†’ notify via SNS

Configuration 
------------------
Event Name: Name of the Event 
Prefix    : Limit the notification  to objects with key starting with spcified character
Suffix    : Limit the notification to Objects with key ending with specified character
Event type: Specify at least one event for which you want to receive notifications.
            For each group, you can choose an event type for all events, or
            you can choose one or more individual events.
Some of the event type are: 
*Object creation 
*Object remomval 
*Object restored
*Object ACL
*Object tagging
*Reduced Redundancy storage
*Replicatoin 
*Lifecycle
*Intelligent tiering

Destination : 
Here you choose the destination LIKE Lambda,SNS,SQS  and specify what to do when an event occur 
9.AWS CloudTrail
----------------------
You can view and configure CloudTrail data events for Amazon S3 bucket object-level operations 
in the AWS CloudTrail console.
10.Sever logging 
-----------------------
it is a feature that tracks every request made to your bucket
whne enabled ,S3 will record logs for :
*Who accessed your bucket
*What operation they performed
*When they accessed it
*From what IP address 
*Whether access was allowed or denied 
These logs are stored in another S3 bucket  a loggin target bucket
 Of which you should provide the path and  and log object key format
11.Intelligent Tirering Archive Configuration 
----------------------------------------------
S3 intelligent-tiering automatically moves your objects between tires based on how ofter they are accessed ,
to save you mony 
The Archive configuration specifically controls deeper cold tiers
*Archive  Access tire -> Cheap ,slower
*Deep Archive Access tire -> Super cheap ,very slow 

Configuration:
Configuration name:  Enter the configuration name 
Choose a configuration scope 
>> Limit the scope of this configuration using one or more filters
   This configuration applies to all objects in the bucket
You can also do this by using the : Object tags 

Status :  Help's to choose wheather the configutaion will be enabled or disabled

Archive rule aciton's:
Intelligent-Tiering can tier down objects to the Archive Access tier, the Deep Archive Access tier, or both. 
The number of days until transition to the selected tiers can be extended up to a total of 2 years. Learn more 

Archive Access tier
When enabled, Intelligent-Tiering will automatically move objects that haven't been accessed for a minimum of 90 days to the Archive Access tier.
Deep Archive Access tier
When enabled, Intelligent-Tiering will automatically move objects that haven't been accessed for a minimum of 180 days to the Deep Archive Access tier.
12.Encryption: 
-------------------------------------
Server-side encryption is automatically applied to new objects stored in this bucket.
12.Bucket ABAC
--------------------------------------
Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. 
With ABAC, you can attach tags to your general purpose buckets and AWS Identity and Access Management (IAM)
entities (users or roles), then scale access to objects in your S3 general purpose buckets using tag-based policies.

ABAC status
Disable
Disabling ABAC blocks the ability to manage access using tags.
Enable
Enabling ABAC allows you to manage access to buckets with tags. To manage tags in this bucket, you must have s3:ListTagsForResource, s3:TagResource, and s3:UntagResource permissions.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------















