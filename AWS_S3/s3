Step 1: Creating the S3 bucket
---------------------------------
1.Go to the AWS console  -> S3 bucket ->  Create bucket

*AWS region -> will be taken as per your current place
  - Data will be replicated across multiple AZ's  in this region 
  - Choose the region closer to you or the customer so you will get (Best latency and cheapest for you )

*Bucket type -> General purpose   -> Strandard S3 bucket ,multi-AZ ,Support all storage class
              |_Directory         -> Single AZ, Ultra-low latency , Higher cost

*Bucket Name -> Global name ,used in URL's and API'S 
              |_ No lowercasee,no space ,cannot change later 

*Copie the setting from the existing bucket  : [Only the configuration not the data ]

*Object ownership 
  - ACL's disable  -> YOur account own's all objects and permission via IAM and bucket policies
  - ACzL's enabled -> Objects in thiis bucket can be owned by other AWS account .Access to this bucket and its 
                      objects can be spcificed using ACL's 
                       if you enable this you will be given two options  will pop up 
                       - Bucker owner preffereed  -> If new objects written to this bucket specify the bucket-owner-full-control canned ACL, they are owned by the bucket owner. Otherwise, they are owned by the object writer.
                       - Object writer  -> the object writer remains the object owner
*Block the public access setting to this bucket ->
  - Here you can directly block all public access 
                      or 
  - allow  some access throught the polices 


  Bucker versioning :
                     |_ disable it -> Only the latest version is stored, delete = gone 
                      |_ enable it  -> Keeps all version's  ,protects against overwrite and delete
                                        - Required for the Object lock and S3 backup and PITR
                                         - Increase the storage cost 

Tags :
Meaning : key - value metadata for cost tracking ,Organizatoin ,Automation 

Default encryption : 
 - The Encryption applies automatically to all new objects
    SSE-S3 (Amazon S3 managed keys)  - free
    SSE-KMS                           - uses AWS KMS keys  and Extra cost
    DSSE-KMS                          - Dual encryption ,higher security and higher cost

  Bucket key (Only for SSE KMS)
  -------------------------------
  Reduces KMS call's  -> lower cost
Object Lock
Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock works only in versioned buckets
  
METADATA : 
This feature lets S3 automatically create a database-like  view of your buket content 

Helps to 
*Track every object
*Track every changes 
*Stores that info as queryable table .So you can query with SQL 

What happens is ??
AWS creates managed metadata tabels behind the scenes:
*Stored in another AWS-managed S3 bucket
*Format : Apache Iceberg
*Continuously update ,You do NOT need to manage these tabels manually 


Metadata configuration :
Once enabled :
*Every upload / delete / overwrite is tracked 
*Metadata tabels auto-update

What can you do with it :
*You can Run SQL queries using : 
  - Amazon Athena 
  - Qucick sight 
  - Apache spark 

Ex:
SELECT * FROM bucket_metadata
WHERE size > 100MB
AND last_modified > now() - 7 days;

Types of metadata : 

1.Journal Tabel
-----------------------
What it stores: 
*Object changes over time 
*Upload 
*Deletes 
*UPdates

Use cases:
*Auditing 
*Security 
*Compliance
*Who changed what and when 

Think of it like a cloudTrial of the S3 objects

Record expiration : 
*Disable : History is kept forever 
*Enable  : auto delete old records (cheaper )

2.Live inventory Tabel 
-------------------------------
What it stores
*Current state of ALL objects
*One row per object

Use cases: 
*Big data jobs
*ML pipelines
*Batch processing 
*Finding files quickly

Think of it like a :
>> ls -l for your bucket  - but instant and queryable

Keep the encryption default unless the job needs strict security and compliance

______________________________________________________________________________________________________________
ACCESS POINT :
An S3 Access point is an separate ,named way to access an S3 bucket with its own permission and network rules \
Used for API access such as (CLI/SDK/backend apps)
It not for the static website


It doesn't store the data 
It Controls  how the data is accessed 

Normal bucket access: 
---------------------------------
 App / User
    |
 S3 bucket

*Only bucket policy 
*One big set of permission 
*Hard to manage  when many app's / users exits

Bucket access with Access point 
--------------------------------

App A â†’ Access Point A â†’ Bucket
App B â†’ Access Point B â†’ Bucket

*Same bucket 
*Multiple access point 
*Each access point  : 
  - has its own  Access policy 
  - has its own  Network policy 

Access point configuration :
1.Give the accesss point a name:   
2.Network origin 
  Here you have two options: 
  - Virtual private cloud ->  You should give the VPC ID   [ No internet access .Request are made over a spcidfied VPC only ]
  - Internet              ->  The access point has a public endpoint  .Can be accessed from the Intenet ,AWS cli ,SDK's 
                               -Still not public by default 
                               - Requires authentication (IAM/ signed request)
3.Block Public Access Settings 
You selected ; 
Block all public access 
This means : 
*Nobody can make this access point public
*Even if someone tries to add:
-Public ACLs
-Public policie

Access Point policy (optional)
-------------------------------------
This is a JSON policy that controls:
*Who can access the bucket
*What they can do
*ONLY via this access point

Example use cases:

- Read-only access
- Upload-only access
- Restrict to one IAM role

If you leave it empty:

*Access is controlled by:
-IAM permissions
-Bucket policy

ðŸ“Œ Access is allowed only if BOTH allow it
_____________________________________________________________________________________________________________
MANAGEMENT
-------------

Lifecycle rules
-----------------
Lifecycle rules tells the S3 what to do with objects automatically as they get older  
- move them to cheaper storage or delete them 

LifeCycle rule name:
---------------------
Name: Lifecycle rule name
=> Give the name that suits 

Choose the rule scope :
This decides which object the rules applies

*Apply to all objects in the buckets
 - Every file in the bukcet
 - Simple but dangerous if used carelessly 

*Limit the scope using the filter 
 -You an target specific target 

Filter type: 
Prefix:[Ex : logs/,uploads/,temp/]
Meaning : only the objects whose key start with that prefix
Ex: 
- logs/app.logs
- logs/2026/a.logs
- images/pic.jpg   -> this will not be included as it doesn't start with any of the prefix

Object tags: 
----------------
env = dev
type  = temp

Meaning object with thesse tags  ,this allow a very precise  control  .Used a lot in production 

Object size:
-------------------
*Minimux / Maximum size of the object
 Useful for :
  *large backup 
  *small temp files

Lifecycle rule action : Choose the actions you want this rule to perform 

1.Transition  current version of objects between the storage classes
   Means : choose the transition to move current version's of objects between storage classes based on your u
            use case scenario and performance access requirementes .These transitoins start from when the objects
             are created and ocnsecutively applied 
 
This will ask for a Storage class you want to transition to  and the days after which this data will be  transitioned
2.Transition non currnet  version's of objects between storage classes [This will applie for non-versioned objects ]
3.Expire  current version of objects 
  Means : if the version is enabled  then when the expiration date is reached  
          1. S3 does not delete the object data
          2. s3 adds a delete marker 
          3.The object disappears  form the console, Normal GET , Website access 
             But 
                *The actual files become a non-current version 
                *You are still charged for the storage 
         Expire the current version and permanently delets the non current version 

          if the versioning is disabled  and the expiration data is reached 
          1.Object is permanently deleted
          2.NO delete marker 
          3.No recover
          4.No storage cost after deletion 
This option will ask for the  day's to keep the object from deletion 
4.Permanently delete noncurrent version of objects
  Choose when amazon S3 permanently deletes speified non-current  version objects 
  This will ask for number of days after which the objects become noncurrent and the number of newer version to be retained 
5.Delete expired objects delets markers or incomplete multipart uploads
  Delete expired objects delete marker 
  
When you enable this: 
  *S3 automatically removes those useless delete marker
  *Object metadata is cleaned up 
  *List the operation become faster
  *No data loss 
  
 Incomplete mutlipart uploads 
  *While uploading the large file we often split it into part and uplaod  
  *While uplaoding through parts if the upload fail or interrupted ro abandoned  the corrupted part will stay in the s3 and you will be charged for it
 But when you  enable this  deletes multipart uploads that have been incomplete for X days
_________________________________________________________________________________________________________________
Replication 

S3 replication  = automatic  copying of the objects from one bucket to another 
It works : 
*Within the same region  (SRR - Same region replication )
*Across region           (CRR Cross regio replication )

Whenever an object is : 
*Uploaded
*Updated
*Deleted 
S3 automatically copies that change to the destination bucket
** This replication only works when  the versioning is enabled so that its easy to trackt the changes 

Configuring the  Replication Rule screen : 
1.Replication rule Name :  Give a name for the replication rule
2.Choose wheather the  rule will be enabled or disabled  when created 
  Means : Enabled -> replication start's immediately 
          Disabled -> rule exits but does nothing 
3.Prioirty:
  Used only if : 
  *You have multiple replication rules 
  *Same object matches more that one rule 
Lower the number higher the priority 
 
4. The Rule Scope is similar to lifecycle
5.Destination section : 
  Where the copy goes:*same account, different account 
6.IAM role : 
   Provide  permission to access the spcified resources 
7.Encryption : Inside this you have the option to 
               Enable : Replicate objects  encrypted with AWS key management services 
8.Destination storage class : 
   Change the storage class of the s3 where the replicated data will be kept 
9. Additional replication options :
   Replication Time Control (RTC)
   - Replication Time Control replicates 99.99% of new objects within 15 minutes and 
      includes replication metrics. Additional fees will apply. 
    Delete marker replication
    - Delete markers created by S3 delete operations will be replicated.
      Delete markers created by lifecycle rules are not replicated.
    Replica modification sync
    - Replicate metadata changes made to replicas from the destination bucket to the source bucket.
    Replication metrics
    - With replication metrics, you can monitor the total number and size of objects that are pending replication,
      and the maximum replication time to the destination Region. 
      You can also view and diagnose replication failures. CloudWatch metrics fees apply.


____________________________________________________________________________________________________________________________


















